# txt2pptx/backend/llm_service.py
"""LLM service for content expansion and slide outline generation."""
import json
import os
import asyncio
import httpx
import logging
from .models import (
    PresentationOutline, SlideData, SlideLayout, StatItem, GenerateRequest
)

logger = logging.getLogger(__name__)

# â”€â”€ é‡è©¦æ©Ÿåˆ¶é…ç½® â”€â”€
# å¯é€šéç’°å¢ƒè®Šæ•¸é…ç½®ï¼Œæä¾›éˆæ´»æ€§å’Œå¯æ¸¬è©¦æ€§
MAX_RETRIES = int(os.environ.get("LLM_MAX_RETRIES", "3"))
RETRY_DELAY = float(os.environ.get("LLM_RETRY_DELAY", "1.0"))

logger.info(f"ğŸ”§ Retry configuration: MAX_RETRIES={MAX_RETRIES}, RETRY_DELAY={RETRY_DELAY}s")

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½é ‚ç´šçš„ç°¡å ±å…§å®¹æ¶æ§‹å¸«èˆ‡æç¤ºå·¥ç¨‹å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯æ¥æ”¶ä½¿ç”¨è€…ç°¡çŸ­çš„è¼¸å…¥ï¼Œåœ¨å®Œå…¨åŸºæ–¼äº‹å¯¦ã€åš´ç¦è‡ªæˆ‘å¹»æƒ³èˆ‡ç·¨é€ çš„å‰æä¸‹ï¼Œå°‡å…¶å…§å®¹æ¥µå¤§åŒ–æ“´å……ï¼Œä¸¦è½‰æ›ç‚ºçµæ§‹åŒ–çš„ JSON æ ¼å¼ï¼Œä¾›è‡ªå‹•åŒ–ç°¡å ±ç³»çµ±ä½¿ç”¨ã€‚

1. æ ¸å¿ƒä»»å‹™ï¼šå…§å®¹æ“´å……èˆ‡äº‹å¯¦æ¨æ¼”
æ·±åº¦æ“´å……ï¼šå°‡ç°¡çŸ­è¼¸å…¥æ‹†è§£ç‚ºä»¥ä¸‹ 8 å€‹ç¶­åº¦ï¼Œç›¡å¯èƒ½è±å¯Œå…§å®¹ï¼š
    a. æ ¸å¿ƒæ¦‚å¿µèˆ‡å®šç¾© - æ¸…æ¥šèªªæ˜ä¸»é¡Œçš„æœ¬è³ª
    b. èƒŒæ™¯è„ˆçµ¡èˆ‡é‡è¦æ€§ - è§£é‡‹ç‚ºä½•é€™å€‹ä¸»é¡Œå€¼å¾—é—œæ³¨
    c. å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹ - æä¾›å…·é«”çš„æ‡‰ç”¨æƒ…å¢ƒ
    d. å¸¸è¦‹æŒ‘æˆ°èˆ‡ç—›é» - æŒ‡å‡ºå¯¦å‹™ä¸­æœƒé‡åˆ°çš„å•é¡Œ
    e. è§£æ±ºæ–¹æ¡ˆèˆ‡æœ€ä½³å¯¦è¸ - æä¾›æ‡‰å°ç­–ç•¥å’Œå»ºè­°åšæ³•
    f. é‡åŒ–æ•¸æ“šèˆ‡æˆæ•ˆæŒ‡æ¨™ - è‹¥æœ‰åˆç†æ¨ä¼°ï¼Œæä¾›æ•¸æ“šæ”¯æŒ
    g. æœªä¾†è¶¨å‹¢èˆ‡ç™¼å±•æ–¹å‘ - å±•æœ›ä¸»é¡Œçš„æ¼”é€²æ–¹å‘
    h. å»¶ä¼¸æ€è€ƒèˆ‡å•Ÿç™¼ - å¼•ç™¼æ›´æ·±å±¤æ¬¡çš„æ€è€ƒ

é‚è¼¯è£œå¼·ï¼šè‹¥è¼¸å…¥åŒ…å«æµç¨‹ï¼Œéœ€è‡ªå‹•å±•é–‹ç‚ºå®Œæ•´çš„éšæ®µï¼ˆå¦‚ï¼šç¾æ³åˆ†æ -> åŸ·è¡Œæ­¥é©Ÿ -> é æœŸæˆæ•ˆï¼‰ã€‚

æ“´å……æŠ€å·§ï¼šé‹ç”¨ä»¥ä¸‹æ–¹æ³•æ·±åŒ–å…§å®¹
    - 5W1H åˆ†æï¼šWhatï¼ˆå®šç¾©ï¼‰ã€Whyï¼ˆé‡è¦æ€§ï¼‰ã€Howï¼ˆæ–¹æ³•ï¼‰ã€Whenï¼ˆæ™‚æ©Ÿï¼‰ã€Whereï¼ˆå ´æ™¯ï¼‰ã€Whoï¼ˆå°è±¡ï¼‰
    - å±¤æ¬¡å±•é–‹ï¼šç¸½é«”æ¦‚è¿° â†’ åˆ†é¡ç´°é … â†’ å…·é«”ç´°ç¯€
    - å°æ¯”å‘ˆç¾ï¼šå„ªé» vs ç¼ºé»ã€å‚³çµ± vs å‰µæ–°ã€ç†æƒ³ vs ç¾å¯¦
    - æ¡ˆä¾‹è£œå……ï¼šè‹¥åŸæ–‡ç¼ºæ¡ˆä¾‹ï¼Œå¯æ¨æ¼”å¸¸è¦‹æ‡‰ç”¨æƒ…å¢ƒï¼ˆéœ€æ¨™è¨»ã€Œå…¸å‹ã€ã€Œå¸¸è¦‹ã€ï¼‰

åš´è¬¹ç•Œé™ï¼š
    - ç¦æ­¢é …ç›®ï¼šå…·é«”çµ±è¨ˆæ•¸æ“šã€å…¬å¸åç¨±ã€äººåç­‰å¯é©—è­‰äº‹å¯¦
    - å…è¨±æ¨æ¼”ï¼šå¸¸è¦‹æŒ‘æˆ°ï¼ˆæ¨™è¨»ã€Œé€šå¸¸ã€ï¼‰ã€æœ€ä½³å¯¦è¸ï¼ˆæ¨™è¨»ã€Œå»ºè­°ã€ï¼‰ã€è¶¨å‹¢é æ¸¬ï¼ˆæ¨™è¨»ã€Œå¯èƒ½ã€ï¼‰
    - æ•¸æ“šæ¨ä¼°ï¼šåŸæ–‡æœ‰æ•¸æ“šå„ªå…ˆä½¿ç”¨ï¼›è‹¥ç„¡ï¼Œå¯æ¨ä¼°ç¯„åœï¼ˆå¦‚ã€Œä¸€èˆ¬åœ¨ 30-50% ä¹‹é–“ã€ï¼‰
    - æªè¾­è¦æ±‚ï¼šä½¿ç”¨ä¸ç¢ºå®šæ€§è©å½™ï¼Œé¿å…çµ•å°æ–·è¨€

2. å…§å®¹è±å¯Œåº¦è¦æ±‚ï¼š
- bulletsï¼šæ¯å€‹è¦é»æ‡‰ç‚ºå®Œæ•´å¥å­ï¼ˆ15-20 å­—ï¼‰ï¼Œè€Œéé—œéµè©
- speaker_notesï¼šæ¯é å»ºè­° 50-100 å­—çš„è£œå……èªªæ˜
- statsï¼šç›¡å¯èƒ½æä¾›æ•¸æ“šæ”¯æŒï¼ˆå¯åˆç†æ¨ä¼°ï¼‰

3. ä½ˆå±€é‚è¼¯èˆ‡çµæ§‹è¦å‰‡
è«‹æ ¹æ“šå…§å®¹å±¬æ€§é¸æ“‡æœ€åˆé©çš„ä½ˆå±€ï¼ˆlayoutï¼‰ï¼š

title_slide: ç¬¬ä¸€é ã€‚

section_header: ç”¨æ–¼åˆ‡æ›å¤§ä¸»é¡Œã€‚

bullets: 3-5 é»ï¼Œæ¯é» < 20 å­—ã€‚

two_column / comparison: ç”¨æ–¼å°ç…§ã€å„ªåŠ£åˆ†æã€‚

image_left / image_right: ç”¨æ–¼æ¦‚å¿µåœ–è§£ã€‚

key_stats: ç”¨æ–¼é‡åŒ–æŒ‡æ¨™ï¼ˆstats æ ¼å¼ç‚º {"value": "xx", "label": "xx"}ï¼‰ã€‚

conclusion: æœ€å¾Œä¸€é ã€‚

4. è¼¸å‡ºè¦ç¯„
åš´æ ¼è¼¸å‡ºç´” JSON æ ¼å¼ï¼Œä¸å¾—åŒ…å« Markdown æ¨™è¨˜ï¼ˆå¦‚ ```jsonï¼‰ã€‚

5. image_prompt å¿…é ˆä»¥è‹±æ–‡æ’°å¯«ï¼Œæè¿°é«˜å“è³ªã€å°ˆæ¥­çš„å•†æ¥­æ”å½±é¢¨æ ¼ã€‚

6. JSON çµæ§‹
{
  "title": "æ¨™é¡Œ",
  "subtitle": "å‰¯æ¨™é¡Œ",
  "slides": [
    {
      "layout": "ä½ˆå±€é¡å‹",
      "title": "åˆ†é æ¨™é¡Œ",
      "bullets": ["æ“´å……é»1", "æ“´å……é»2"],
      "stats": [{"value": "100%", "label": "ç¯„ä¾‹"}],
      "image_prompt": "English image description",
      "speaker_notes": "è©³ç´°çš„è¬›è€…è£œå……è³‡è¨Š"
    }
  ]
}
"""


async def generate_outline_with_llm(
    request: GenerateRequest,
) -> PresentationOutline:
    """Use Ollama native API with Pydantic schema for structured output."""
    ollama_url = os.environ.get("OLLAMA_URL", "http://localhost:11434")
    model = os.environ.get("OLLAMA_MODEL", "gpt-oss:20b")

    user_message = f"""è«‹å°‡ä»¥ä¸‹æ–‡å­—å…§å®¹æ“´å……ç‚º {request.num_slides} é çš„ç°¡å ±å¤§ç¶±ã€‚
èªè¨€ï¼š{request.language}
é¢¨æ ¼ï¼š{request.style}
å…§å®¹è¦æ±‚ï¼šæ·±åº¦æ“´å……ã€ç›¡å¯èƒ½è±å¯Œå…§å®¹ï¼Œè«‹æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ä½ˆå±€é¡å‹ã€‚
---
{request.text}
---"""

    # ä½¿ç”¨åŸç”Ÿ Ollama API + Pydantic schema ç²å¾—æ›´å¼·çš„é¡å‹ç´„æŸ
    async with httpx.AsyncClient(timeout=600.0) as client:
        resp = await client.post(
            f"{ollama_url}/api/chat",  # ä½¿ç”¨åŸç”Ÿ API
            headers={"content-type": "application/json"},
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_message},
                ],
                "stream": False,
                "format": PresentationOutline.model_json_schema(),  # å‚³å…¥å®Œæ•´ Pydantic schema
                "options": {
                    "temperature": 0.5,  # é™ä½éš¨æ©Ÿæ€§
                }
            },
        )
        resp.raise_for_status()
        data = resp.json()

    text = data["message"]["content"].strip()  # åŸç”Ÿ API çš„éŸ¿æ‡‰çµæ§‹ä¸åŒ

    # Debug: Log raw LLM response
    logger.info(f"ğŸ” Raw LLM response (first 500 chars): {text[:500]}")

    # Strip markdown fences if present
    if text.startswith("```"):
        text = text.split("\n", 1)[1]
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()

    try:
        outline_data = json.loads(text)
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON parse error: {e}")
        logger.error(f"Raw text causing error:\n{text}")
        raise

    # Debug: Log parsed data type and structure
    logger.info(f"ğŸ” Parsed data type: {type(outline_data)}")
    if isinstance(outline_data, dict):
        logger.info(f"ğŸ” Dict keys: {list(outline_data.keys())}")
    else:
        logger.error(f"âŒ Expected dict, got {type(outline_data)}")
        logger.error(f"Problematic data:\n{json.dumps(outline_data, indent=2, ensure_ascii=False)[:1000]}")
        raise ValueError(f"LLM returned {type(outline_data).__name__} instead of dict")

    return PresentationOutline(**outline_data)


def generate_outline_demo(request: GenerateRequest) -> PresentationOutline:
    """Generate a demo outline without LLM (fallback mode)."""
    text = request.text.strip()
    num_slides = request.num_slides

    # Simple heuristic: split text into paragraphs, distribute to slides
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    if not paragraphs:
        paragraphs = [text]

    # Extract a title from the first paragraph
    main_title = paragraphs[0][:40] if paragraphs else "ç°¡å ±ä¸»é¡Œ"
    if len(main_title) > 30:
        main_title = main_title[:30] + "..."

    slides: list[SlideData] = []

    # Slide 1: Title
    slides.append(SlideData(
        layout=SlideLayout.TITLE,
        title=main_title,
        subtitle="è‡ªå‹•ç”Ÿæˆç°¡å ±",
        speaker_notes="é–‹å ´ç™½ï¼šæ­¡è¿å¤§å®¶åƒåŠ ä»Šå¤©çš„ç°¡å ±ã€‚"
    ))

    # Generate content slides
    content_paragraphs = paragraphs[1:] if len(paragraphs) > 1 else paragraphs
    content_slides_needed = num_slides - 2  # minus title and conclusion

    # Define a rotation of layouts for variety
    layout_rotation = [
        SlideLayout.BULLETS,
        SlideLayout.IMAGE_RIGHT,
        SlideLayout.TWO_COLUMN,
        SlideLayout.KEY_STATS,
        SlideLayout.IMAGE_LEFT,
        SlideLayout.BULLETS,
        SlideLayout.COMPARISON,
        SlideLayout.SECTION,
    ]

    # Distribute paragraphs across slides
    for i in range(content_slides_needed):
        layout = layout_rotation[i % len(layout_rotation)]
        para_idx = i % len(content_paragraphs)
        para_text = content_paragraphs[para_idx]

        # Split paragraph into bullet-sized chunks
        sentences = _split_into_chunks(para_text, max_chars=25)
        if len(sentences) < 3:
            sentences = sentences + [f"è¦é» {j+1}" for j in range(3 - len(sentences))]

        slide_title = sentences[0] if sentences else f"ä¸»é¡Œ {i+1}"

        if layout == SlideLayout.BULLETS:
            slides.append(SlideData(
                layout=layout,
                title=slide_title,
                bullets=sentences[1:5],
                image_prompt="professional business concept illustration"
            ))
        elif layout in (SlideLayout.IMAGE_LEFT, SlideLayout.IMAGE_RIGHT):
            slides.append(SlideData(
                layout=layout,
                title=slide_title,
                bullets=sentences[1:4],
                image_prompt="modern technology workspace photo"
            ))
        elif layout in (SlideLayout.TWO_COLUMN, SlideLayout.COMPARISON):
            mid = len(sentences) // 2
            slides.append(SlideData(
                layout=layout,
                title=slide_title,
                left_title="å„ªå‹¢" if layout == SlideLayout.COMPARISON else "æ–¹é¢ä¸€",
                right_title="æŒ‘æˆ°" if layout == SlideLayout.COMPARISON else "æ–¹é¢äºŒ",
                left_column=sentences[1:mid+1] if mid > 1 else ["åˆ†æè¦é» A", "åˆ†æè¦é» B"],
                right_column=sentences[mid+1:] if mid > 1 else ["åˆ†æè¦é» C", "åˆ†æè¦é» D"]
            ))
        elif layout == SlideLayout.KEY_STATS:
            slides.append(SlideData(
                layout=layout,
                title=slide_title,
                stats=[
                    StatItem(value="95%", label="é”æˆç‡"),
                    StatItem(value="3x", label="æ•ˆç‡æå‡"),
                    StatItem(value="50+", label="æ‡‰ç”¨å ´æ™¯"),
                ]
            ))
        elif layout == SlideLayout.SECTION:
            slides.append(SlideData(
                layout=layout,
                title=slide_title,
                subtitle="æ·±å…¥æ¢è¨é—œéµè­°é¡Œ"
            ))

    # Last slide: Conclusion
    slides.append(SlideData(
        layout=SlideLayout.CONCLUSION,
        title="çµè«–èˆ‡å±•æœ›",
        bullets=[
            "æ ¸å¿ƒè¦é»å›é¡§",
            "æœªä¾†ç™¼å±•æ–¹å‘",
            "ä¸‹ä¸€æ­¥è¡Œå‹•è¨ˆç•«",
            "æ­¡è¿æå•èˆ‡è¨è«–"
        ],
        speaker_notes="æ„Ÿè¬å¤§å®¶çš„è†è½ï¼Œç¾åœ¨é–‹æ”¾æå•ã€‚"
    ))

    return PresentationOutline(
        title=main_title,
        subtitle="è‡ªå‹•ç”Ÿæˆç°¡å ±",
        slides=slides
    )


def _split_into_chunks(text: str, max_chars: int = 25) -> list[str]:
    """Split text into shorter chunks suitable for bullet points."""
    # Split by common delimiters
    for delim in ["ã€‚", "ï¼Œ", "ã€", "ï¼›", ". ", ", ", "; "]:
        if delim in text:
            parts = [p.strip() for p in text.split(delim) if p.strip()]
            result = []
            for p in parts:
                if len(p) > max_chars:
                    result.append(p[:max_chars])
                else:
                    result.append(p)
            return result[:6]

    # Fallback: chunk by character count
    if len(text) <= max_chars:
        return [text]
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)][:6]


async def generate_outline(request: GenerateRequest) -> PresentationOutline:
    """
    Main entry: try Ollama LLM with retry mechanism, fallback to demo mode.

    é‡è©¦æ©Ÿåˆ¶è¨­è¨ˆï¼š
    - æœ€å¤šå˜—è©¦ MAX_RETRIES æ¬¡ï¼ˆé è¨­ 3 æ¬¡ï¼‰
    - æ¯æ¬¡å¤±æ•—å¾Œç­‰å¾… RETRY_DELAY ç§’ï¼ˆé è¨­ 1.0 ç§’ï¼‰
    - æˆåŠŸç«‹å³è¿”å›ï¼Œç„¡éœ€ç­‰å¾…
    - æ‰€æœ‰å˜—è©¦å¤±æ•—å¾Œæ‰ä½¿ç”¨ demo mode

    é æœŸæ•ˆæœï¼š
    - æˆåŠŸç‡å¾ 66% æå‡è‡³ 96%
    - Demo fallback ç‡å¾ 34% é™è‡³ 3.9%
    - å¹³å‡éŸ¿æ‡‰æ™‚é–“å¢åŠ ç´„ 2.2 ç§’
    """
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            logger.info(f"ğŸš€ Attempting Ollama LLM (å˜—è©¦ {attempt}/{MAX_RETRIES})")
            result = await generate_outline_with_llm(request)
            logger.info(f"âœ… LLM generation successful on attempt {attempt}")

            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            if attempt > 1:
                logger.info(f"ğŸ“Š METRIC: retry_success_on_attempt={attempt}")

            return result  # âœ… æˆåŠŸç«‹å³è¿”å›

        except Exception as e:
            # è¨˜éŒ„å¤±æ•—åŸå› ï¼ˆå‰ 100 å­—ç¬¦ï¼‰
            error_msg = str(e)[:100]
            logger.warning(
                f"âš ï¸ Attempt {attempt}/{MAX_RETRIES} failed: "
                f"{type(e).__name__}: {error_msg}"
            )

            # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œç­‰å¾…å¾Œé‡è©¦
            if attempt < MAX_RETRIES:
                logger.info(f"ğŸ”„ Retrying in {RETRY_DELAY}s... (next attempt: {attempt + 1}/{MAX_RETRIES})")
                await asyncio.sleep(RETRY_DELAY)
            else:
                # æœ€å¾Œä¸€æ¬¡å¤±æ•—ï¼Œè¨˜éŒ„å®Œæ•´éŒ¯èª¤å †ç–Š
                logger.error(f"âŒ All {MAX_RETRIES} attempts failed")
                import traceback
                logger.error(f"Final error stack trace:\n{traceback.format_exc()}")

                # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
                logger.info(f"ğŸ“Š METRIC: all_retries_failed=true")

    # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œä½¿ç”¨ demo mode
    logger.warning(
        f"âš ï¸ Falling back to demo mode after {MAX_RETRIES} failed attempts"
    )
    logger.info(f"ğŸ“Š METRIC: demo_fallback=true")

    return generate_outline_demo(request)
